{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PczSFmOQ1uVg",
        "outputId": "43a2af2e-3e43-456e-84e4-b8a87998b6e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import gdown\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this section downloads the pre-processed file to user environemnt in colab\n",
        "path = 'https://drive.google.com/file/d/1fPKeAyTcvc-txkxWy-Px28BGMnSVzKWY/view?usp=share_link'\n",
        "id = path.split('/d/')[1].split('/')[0]\n",
        "link = 'https://drive.google.com/uc?export=download&id='+id\n",
        "name = 'datafile.json'\n",
        "gdown.download(link, name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "zvj9t_epX5mC",
        "outputId": "295aba8d-36fd-4b57-cfbd-cf4b2fe1b30e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1fPKeAyTcvc-txkxWy-Px28BGMnSVzKWY\n",
            "To: /content/datafile.json\n",
            "100%|██████████| 88.3M/88.3M [00:00<00:00, 187MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'datafile.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rank_bm25\n",
        "!pip install streamlit\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d9tpMzkFg1a",
        "outputId": "9a8bc302-5dc3-4acf-f145-8591d7966e4e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rank_bm25) (1.25.2)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.33.0-py2.py3-none-any.whl (8.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.0)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.2.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.10.0)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.8.1b0 smmap-5.0.1 streamlit-1.33.0 watchdog-4.0.0\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add athirosation token to ngrok service access\n",
        "import streamlit as st\n",
        "from pyngrok import ngrok\n",
        "\n",
        "!ngrok config add-authtoken 2eBgswpP1QdUxillV6eeWhPF7Q0_6XoyysKL7fGVB45hNTsWg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFj6Y2iTxANQ",
        "outputId": "84d51eff-37fc-4a13-e4b6-c86784b6bf3b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from pyngrok import ngrok\n",
        "import json\n",
        "import string\n",
        "import nltk\n",
        "import requests\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from rank_bm25 import BM25Okapi\n",
        "import gensim\n",
        "from gensim import corpora, models, similarities\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def createIndex():\n",
        "  path = '/content/datafile.json'\n",
        "  tokenized_corpus, data, dictionary = get_data(path)\n",
        "  bm25 = getBM25(tokenized_corpus)\n",
        "  a, b, c = get_LSI_space(tokenized_corpus)\n",
        "  return a, b, c, dictionary, data, bm25\n",
        "\n",
        "def get_data(path):\n",
        "  with open(path,'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "  documents = list(data.values())\n",
        "  indices = list(data.keys())\n",
        "  tokenized_corpus = [doc.split(\" \") for doc in documents]\n",
        "  dictionary = corpora.Dictionary(tokenized_corpus)\n",
        "\n",
        "  return tokenized_corpus, data, dictionary\n",
        "\n",
        "def getBM25(tokenized_corpus):\n",
        "  k1 = 11.\n",
        "  b = 1.\n",
        "  epsilon = 0.25\n",
        "  bm25 = BM25Okapi(tokenized_corpus, k1=k1, b=b, epsilon=epsilon)\n",
        "  return bm25\n",
        "\n",
        "def get_LSI_space(tokenized_corpus):\n",
        "  # Create a Dictionary object that maps each word to a unique id, using a tokenized version of the corpus.\n",
        "  dictionary = corpora.Dictionary(tokenized_corpus)\n",
        "\n",
        "  # Convert the tokenized corpus into a bag-of-words (BoW) format using the previously created dictionary.\n",
        "  corpus = [dictionary.doc2bow(text) for text in tokenized_corpus]\n",
        "  tfidf = models.TfidfModel(corpus)\n",
        "  corpus_tfidf = tfidf[corpus]\n",
        "  lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=200)\n",
        "  lsi_corpus = lsi_model[corpus_tfidf]\n",
        "\n",
        "  return lsi_corpus, corpus, lsi_model\n",
        "\n",
        "def get_LSI_result(query, corpus, dictionary, lsi_model):\n",
        "  tokens = word_tokenize(query.lower())\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  stripped = [w.translate(table) for w in tokens]\n",
        "  words = [word for word in stripped if word.isalpha()]\n",
        "  query = ' '.join(words)\n",
        "  tokenized_query = word_tokenize(query.lower())\n",
        "  query_bow = dictionary.doc2bow(tokenized_query)\n",
        "  tfidf = models.TfidfModel(corpus)\n",
        "  query_tfidf = tfidf[query_bow]\n",
        "  query_lsi = lsi_model[query_tfidf]\n",
        "\n",
        "  return query_lsi # (query, corpus, dictionary)\n",
        "\n",
        "def bm25_result(query, bm25, nct_ids):\n",
        "  tokenized_query = word_tokenize(query.lower())\n",
        "  doc_scores = bm25.get_scores(tokenized_query)\n",
        "  bm25_scores_dict = {nct_id: score for nct_id, score in zip(nct_ids, doc_scores)}\n",
        "  return bm25_scores_dict\n",
        "\n",
        "def normScores(bm25_scores_dict, lsi_scores):\n",
        "  # Normalize BM25 scores to be between -1 and 1\n",
        "  max_bm25 = max(bm25_scores_dict.values())\n",
        "  min_bm25 = min(bm25_scores_dict.values())\n",
        "\n",
        "  normalized_bm25_scores = {\n",
        "      nct_id: -1 + 2 * (score - min_bm25) / (max_bm25 - min_bm25)\n",
        "      if max_bm25 != min_bm25 else 0  # To handle the case where all scores are the same\n",
        "      for nct_id, score in bm25_scores_dict.items()\n",
        "  }\n",
        "\n",
        "  max_lsi = max(lsi_scores)\n",
        "  min_lsi = min(lsi_scores)\n",
        "\n",
        "  normalized_lsi_scores = {nct_id: -1 + 2 * (score - min_lsi) / (max_lsi - min_lsi) for nct_id, score in zip(nct_ids, lsi_scores)}\n",
        "\n",
        "  # Score weighing\n",
        "  a1, a2 = 0.55, 0.45\n",
        "  combined_scores_dict = {\n",
        "      nct_id: a1 * normalized_bm25_scores.get(nct_id, 0) + a2 * normalized_lsi_scores.get(nct_id, 0)\n",
        "      for nct_id in set(normalized_bm25_scores) | set(normalized_lsi_scores)\n",
        "  }\n",
        "\n",
        "  return combined_scores_dict\n",
        "\n",
        "placeholder = st.empty()\n",
        "placeholder.write('Building index...')\n",
        "lsi_corpus, corpus, lsi_model, dictionary, data, bm25 = st.cache_data(createIndex)()\n",
        "nct_ids = list(data.keys())\n",
        "search_results = similarities.MatrixSimilarity(lsi_corpus, num_features=lsi_model.num_topics)\n",
        "placeholder.empty()\n",
        "\n",
        "st.title('Latent Semantic Search Engine')\n",
        "st.subheader('Group 11 Coursework Part 3')\n",
        "st.write('Search engine based on latent semantic indexing method, based on the dataset of Clinical Trials Track, consisting of 79,628 documents')\n",
        "st.markdown(\"---\")\n",
        "\n",
        "query = st.text_input('Enter your query:', '')\n",
        "\n",
        "if st.button('Search'):\n",
        "  if query:\n",
        "    st.markdown(\"---\")\n",
        "    placeholder2 = st.empty()\n",
        "    placeholder2.write('Looking for documents matching query...')\n",
        "    lsi_query = get_LSI_result(query, corpus, dictionary, lsi_model)\n",
        "    bm25_scores_dict = bm25_result(query, bm25, nct_ids)\n",
        "    lsi_scores = search_results[lsi_query]\n",
        "\n",
        "    combined_scores_dict = normScores(bm25_scores_dict, lsi_scores)\n",
        "\n",
        "    original_nct_ids = list(data.keys())\n",
        "    ranked_docs_with_scores = sorted(combined_scores_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "    placeholder2.empty()\n",
        "    st.write('Search results:')\n",
        "    for doc_id, score in ranked_docs_with_scores[:10]:\n",
        "      #st.markdown(\"---\")\n",
        "      st.write(f\"Document ID: {doc_id}, Score: {round(score, 4)}\")\n",
        "      st.write(data[doc_id])\n",
        "\n",
        "  else:\n",
        "    st.write('Enter a valid query please')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVGUtTAOpzcR",
        "outputId": "97a01412-4e1d-4d1a-ccb0-379c04384262"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!killall ngrok\n",
        "!streamlit run /content/app.py &>/dev/null&\n",
        "!pgrep streamlit\n",
        "purl = ngrok.connect(8501)"
      ],
      "metadata": {
        "id": "AZO80c2lw1o7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML\n",
        "print('Click on the public url below to proceed to the search engine GUI, on the warning page click Visit Site to continue')\n",
        "link = f'<a href=\"{purl.public_url}\" target=\"_blank\">{purl.public_url}</a>'\n",
        "display(HTML(link))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "WiAj2i4A4_pj",
        "outputId": "a2765da7-390b-4ef3-a5ef-31d2f3db7cf0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Click on the public url below to proceed to the search engine GUI, on the warning page click Visit Site to continue\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"https://74b7-34-123-89-115.ngrok-free.app\" target=\"_blank\">https://74b7-34-123-89-115.ngrok-free.app</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1T3Ci9TqoS7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}