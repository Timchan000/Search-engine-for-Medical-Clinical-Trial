{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Link to download the document set:\n",
        "https://drive.google.com/drive/folders/16vVgaRMY_ZhmrkAWHQTF_aXXYvBulNCN?usp=sharing\n"
      ],
      "metadata": {
        "id": "aNbtdfDCPqqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few examples of the documents:\n",
        "\n",
        "\"NCT00000102\": \"This study will test the ability of extended release nifedipine (Procardia XL), a blood\\r\\n      pressure medication, to permit a decrease in the dose of glucocorticoid medication children\\r\\n      take to treat congenital adrenal hyperplasia (CAH). This protocol is designed to assess both acute and chronic effects of the calcium channel\\r\\n      antagonist, nifedipine, on the hypothalamic-pituitary-adrenal axis in patients with\\r\\n      congenital adrenal hyperplasia. The multicenter trial is composed of two phases and will\\r\\n      involve a double-blind, placebo-controlled parallel design. The goal of Phase I is to examine\\r\\n      the ability of nifedipine vs. placebo to decrease adrenocorticotropic hormone (ACTH) levels,\\r\\n      as well as to begin to assess the dose-dependency of nifedipine effects. The goal of Phase II\\r\\n      is to evaluate the long-term effects of nifedipine; that is, can attenuation of ACTH release\\r\\n      by nifedipine permit a decrease in the dosage of glucocorticoid needed to suppress the HPA\\r\\n      axis? Such a decrease would, in turn, reduce the deleterious effects of glucocorticoid\\r\\n      treatment in CAH.\",\n",
        "\n",
        "\"NCT00000104\": \"Inner city children are at an increased risk for lead overburden. This in turn affects\\r\\n      cognitive functioning. However, the underlying neuropsychological effects of lead overburden\\r\\n      and its age-specific effects have not been well delineated. This study is part of a larger\\r\\n      study on the effects of lead overburden on the development of attention and memory. The\\r\\n      larger study is using a multi-model approach to study the effects of lead overburden on these\\r\\n      effects including the event-related potential (ERP), electrophysiologic measures of attention\\r\\n      and memory are studied. Every eight months, for a total of three sessions the subjects will\\r\\n      complete ERP measures of attention and memory which require them to watch various computer\\r\\n      images while wearing scalp electrodes recording from 11 sites. It is this test that we are\\r\\n      going to be doing on CRC. There will be 30 lead overburdened children recruited from the\\r\\n      larger study for participation in the ERP studies on CRC. These 30 children will be matched\\r\\n      with 30 children without lead overburden. This portion of the study is important in providing\\r\\n      an index of physiological functioning to be used along with behaviorally based measures of\\r\\n      attention and memory, and for providing information about the different measures.\""
      ],
      "metadata": {
        "id": "XW7kH497RCZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "c16yb97zvBL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c87b07-05db-403f-a1d9-ca89b679d8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "    return \" \".join(lemmatized_tokens)\n",
        "\n",
        "json_file_path = \"C:\\\\Users\\\\Timch\\\\nct_summaries_and_descriptions.json\"  # change according to your path, this is the raw documents json file\n",
        "with open(json_file_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "preprocessed_data = {doc_id: preprocess_text(content) for doc_id, content in data.items()}\n",
        "\n"
      ],
      "metadata": {
        "id": "25dIkaUkTcW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_words = \" \".join(preprocessed_data.values()).split()\n",
        "\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "top_10_words = word_counts.most_common(10)\n",
        "\n",
        "top_10_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TijAODBCRb03",
        "outputId": "ad699f41-ea9f-4d4a-8de7-19ed54661e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('patient', 265917),\n",
              " ('study', 230985),\n",
              " ('treatment', 117959),\n",
              " ('day', 69748),\n",
              " ('cell', 58979),\n",
              " ('week', 57766),\n",
              " ('disease', 54928),\n",
              " ('therapy', 53478),\n",
              " ('blood', 53379),\n",
              " ('may', 50796)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_to_remove = [word for word, count in word_counts.most_common(3)]\n",
        "\n",
        "def remove_frequent_words(text, words_to_remove):\n",
        "    tokens = text.split()\n",
        "    filtered_tokens = [word for word in tokens if word not in words_to_remove]\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "adjusted_preprocessed_data = {doc_id: remove_frequent_words(content, words_to_remove) for doc_id, content in preprocessed_data.items()}"
      ],
      "metadata": {
        "id": "2iUkSblOT6ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file_path = 'preprocessed_data.json'\n",
        "with open(output_file_path, 'w') as file:\n",
        "    json.dump(adjusted_preprocessed_data, file, indent=4)\n",
        "\n",
        "print(\"Preprocessing complete. Preprocessed data saved to:\", output_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHPYJithQvTE",
        "outputId": "0a595dee-4ff8-4555-82ae-718abdf34534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete. Preprocessed data saved to: preprocessed_data.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "json_file_path = \"C:\\\\Users\\\\Timch\\\\preprocessed_data.json\"  # change where you want to get the json file\n",
        "\n",
        "with open(json_file_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "num_documents = len(data)\n",
        "\n",
        "print(f\"The JSON file contains {num_documents} documents.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHXxPOVCUApX",
        "outputId": "8b00731a-5ee2-4692-cf22-1fc1fa2ac714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The JSON file contains 79628 documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X3Wtu5rTUH97"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}